{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FirstNeuralNetwork1layer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXP2uMs7SAm+l8tZvRHLmB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emadfrj/NeuralNetwork1layer/blob/main/FirstNeuralNetwork1layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "machin learning NeuralNetwork class without hidden layers\n",
        "\n",
        "**Features:**\n",
        "\n",
        "1.sigmoid function\n",
        "\n",
        "2.square cost function\n",
        "\n",
        "3.bias"
      ],
      "metadata": {
        "id": "TtFNO9qii1w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random as rand\n",
        "\n",
        "class NeuralNetwork1layer:\n",
        "    # batchRelativeVolume define the proportion of mini-batch size to whole dataset\n",
        "    # It is between 0 to 1.\n",
        "    # 0 for stochastic gradient descent and 1 for Batch gradient descent\n",
        "    def __init__(self, learning_rate, batchRelativeVolume):\n",
        "        self.weights = np.array([np.random.randn(), np.random.randn()])\n",
        "        self.bias = np.random.randn()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batchRelativeVolume = batchRelativeVolume\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def _Dsigmoid(self, x):\n",
        "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
        "\n",
        "    def predict(self, input_vector):\n",
        "        layer_1 = np.dot(input_vector, self.weights) + self.bias\n",
        "        layer_2 = self._sigmoid(layer_1)\n",
        "        prediction = layer_2\n",
        "        return prediction\n",
        "\n",
        "    def _compute_gradients(self, input_vector, target):\n",
        "        layer_1 = np.dot(input_vector, self.weights) + self.bias\n",
        "        layer_2 = self._sigmoid(layer_1)\n",
        "        prediction = layer_2\n",
        "\n",
        "        derror_dprediction = 2 * (prediction - target)\n",
        "        dprediction_dlayer1 = self._Dsigmoid(layer_1)\n",
        "        dlayer1_dbias = 1\n",
        "        dlayer1_dweights = (0 * self.weights) + (1 * input_vector)\n",
        "\n",
        "        derror_dbias = (\n",
        "            derror_dprediction * dprediction_dlayer1 * dlayer1_dbias\n",
        "        )\n",
        "        derror_dweights = (\n",
        "            derror_dprediction * dprediction_dlayer1 * dlayer1_dweights\n",
        "        )\n",
        "\n",
        "        return derror_dbias, derror_dweights\n",
        "\n",
        "    def _update_parameters(self, derror_dbias, derror_dweights):\n",
        "        self.bias = self.bias - (derror_dbias * self.learning_rate)\n",
        "        self.weights = self.weights - (\n",
        "            derror_dweights * self.learning_rate\n",
        "        )\n",
        "\n",
        "    def _getbactch (self,input_vectors,targets,RelativeVolume):\n",
        "        if RelativeVolume>=1:\n",
        "            return input_vectors,targets\n",
        "        batch_inputs = []\n",
        "        batch_targets = []\n",
        "        inputLen = len(input_vectors)\n",
        "        numberOfElements = np.floor(RelativeVolume * inputLen).astype(int)\n",
        "        if numberOfElements<=0:\n",
        "            random_data_index = np.random.randint(inputLen)\n",
        "            batch_inputs.append(input_vectors[random_data_index])\n",
        "            batch_targets.append(targets[random_data_index])\n",
        "        else:\n",
        "            random_data_indexes = rand.sample(range(0, inputLen), numberOfElements)\n",
        "            for i in random_data_indexes:\n",
        "                batch_inputs.append(input_vectors[i])\n",
        "                batch_targets.append(targets[i])\n",
        "        return batch_inputs,batch_targets\n",
        "\n",
        "    def train(self, input_vectors, targets, iterations):\n",
        "        cumulative_errors = []\n",
        "        inputLen = len(input_vectors)\n",
        "        for current_iteration in range(iterations):\n",
        "            epoch_input_vectors,epoch_targets = self._getbactch(input_vectors,targets,self.batchRelativeVolume)\n",
        "            epoch_inputLen = len(epoch_input_vectors)\n",
        "            sum_derror_dbias = 0\n",
        "            inputs_dimention = len(epoch_input_vectors[0])\n",
        "            sum_derror_dweights = np.zeros((inputs_dimention))\n",
        "            for index in range(epoch_inputLen):\n",
        "                epoch_input_vector = epoch_input_vectors[index]\n",
        "                epoch_target = epoch_targets[index]\n",
        "                derror_dbias, derror_dweights = self._compute_gradients(epoch_input_vector,epoch_target)\n",
        "                sum_derror_dbias +=derror_dbias\n",
        "                sum_derror_dweights +=derror_dweights\n",
        "            \n",
        "            derror_dbias = sum_derror_dbias / epoch_inputLen\n",
        "            derror_dweights = sum_derror_dweights / epoch_inputLen\n",
        "            self._update_parameters(derror_dbias,derror_dweights)\n",
        "            if current_iteration % 100==0:\n",
        "                cumulative_error = 0\n",
        "                # Loop through all the instances to measure the error\n",
        "                \n",
        "                for data_instance_index in range(inputLen):\n",
        "                    data_point = input_vectors[data_instance_index]\n",
        "                    target = targets[data_instance_index]\n",
        "\n",
        "                    prediction = self.predict(data_point)\n",
        "                    error = np.square(prediction - target)\n",
        "\n",
        "                    cumulative_error = cumulative_error + error\n",
        "                cumulative_errors.append(cumulative_error)\n",
        "        return cumulative_errors"
      ],
      "metadata": {
        "id": "58Guiob0CMFN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of using neural network class for a random dataset"
      ],
      "metadata": {
        "id": "KE7mBDUhjLrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "input_vectors = np.random.normal(size=[100, 2])\n",
        "targets = np.random.normal(size=[100, 1])\n",
        "\n",
        "learning_rate = 0.1\n",
        "# For mini-batch \n",
        "batchRelativeVolume = 0\n",
        "neural_network = NeuralNetwork1layer(learning_rate,batchRelativeVolume)\n",
        "\n",
        "training_error = neural_network.train(input_vectors, targets, 10000)\n",
        "\n",
        "plt.plot(training_error)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Error for all training instances\")\n",
        "plt.savefig(\"cumulative_error.png\")"
      ],
      "metadata": {
        "id": "9HOR7lzxClU5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}